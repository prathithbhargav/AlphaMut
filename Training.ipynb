{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77ba3de0-6a48-48cf-b40a-84d0362ee282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for protein structural modelling\n",
    "from Bio.SVDSuperimposer import SVDSuperimposer\n",
    "import numpy as np\n",
    "import biovec\n",
    "import glob\n",
    "\n",
    "# from utils functions\n",
    "from utils.encoder_decoder import *\n",
    "from utils.sequence import *\n",
    "from utils.reward import *\n",
    "from utils.environment import *\n",
    "\n",
    "# for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# for envronment creation\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "from gymnasium.spaces import Discrete\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "#for reading PDB files and processing them\n",
    "from biopandas.pdb import PandasPdb\n",
    "import pandas as pd\n",
    "from utils.sequence import *\n",
    "\n",
    "# for generating structures through esm instead of modeller\n",
    "import esm\n",
    "import biotite.structure as struc\n",
    "import biotite.structure.io as strucio\n",
    "\n",
    "# for general utility\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27cd3e17-02bf-4f83-882d-e618ef6d10d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork():\n",
    "    def __init__(self, n_state, n_action, n_hidden=50,lr=0.001,entropy_weight=0.01):\n",
    "        self.model = nn.Sequential(nn.Linear(n_state, n_hidden),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(n_hidden, n_action),\n",
    "                                   nn.Softmax(dim=-1), )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "        self.entropy_weight = entropy_weight\n",
    "    def predict(self, s):\n",
    "        return self.model(torch.Tensor(s))\n",
    "    def update(self, returns, log_probs,entropies):\n",
    "        policy_gradient = []\n",
    "        for log_prob, Gt, entropy in zip(log_probs, returns, entropies):\n",
    "            policy_gradient.append((-log_prob * Gt) + (self.entropy_weight * entropy))\n",
    "        loss = torch.stack(policy_gradient).sum()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def get_action(self, s):\n",
    "\n",
    "        probs = self.predict(s)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        log_prob = torch.log(probs[action])\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-9))  # Calculate entropy\n",
    "        return action, log_prob, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88424ef0-a7bb-4349-b072-4c2099098e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def reinforce(env, estimator, n_episode, gamma=1.0):\n",
    "    total_reward_episode = [0] * n_episode\n",
    "\n",
    "    for episode in range(n_episode):\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        entropies = []\n",
    "        state, info,dummy = env.reset()\n",
    "        while True:\n",
    "            action, log_prob, entropy = estimator.get_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward_episode[episode] += reward\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            entropies.append(entropy)\n",
    "            if terminated or truncated:\n",
    "                returns = []\n",
    "                Gt = 0\n",
    "                pw = 0\n",
    "                for reward in rewards[::-1]:\n",
    "                    Gt += gamma ** pw * reward\n",
    "                    pw += 1\n",
    "                    returns.append(Gt)\n",
    "                returns = returns[::-1]\n",
    "                returns = torch.tensor(returns)\n",
    "                if returns.std() != 0:\n",
    "                    returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "                estimator.update(returns, log_probs,entropies)\n",
    "                print('Episode: {}, total reward: {}'.format(episode, total_reward_episode[episode]))\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "    return total_reward_episode\n",
    "                \n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db622bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312a0c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PeptideEvolution(folder_containing_pdb_files='../DrugResistance/folder_for_machine_learning/30_test/',\n",
    "                       structure_generator='esm_sse',\n",
    "                       validation=False,\n",
    "                       reward_cutoff=50,\n",
    "                       unique_path_to_give_for_file = 'unique_1',\n",
    "                       maximum_number_of_allowed_mutations_per_episode=15,\n",
    "                       folder_to_save_validation_files='validation_structures',use_proline=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b430629d-4c2c-473a-9ff1-92fbe5e095d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "n_hidden = 128\n",
    "lr = 0.0007\n",
    "gamma = 0.95\n",
    "entropic_factor = 0.0\n",
    "n_episode = 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f45651",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(n_state, n_action, n_hidden, lr,entropy_weight=entropic_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09b561c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'protein_to_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_157124/2697161385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_reward_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_episode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_157124/2438317316.py\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(env, estimator, n_episode, gamma)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mentropies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/evolution/helix_breaker/utils/environment.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0minitial_pdb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_of_template_pdb_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# initial_pdb_path = self.folder_of_initial_pdb_structures+'/'+random.choice(list_of_initial_pdb_files)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0minitial_pdb_structure_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprotein_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_pdb_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_pdb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy_state_for_mutator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprotein_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_pdb_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_pdb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_sequence_to_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_pdb_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_pdb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'protein_to_indices' is not defined"
     ]
    }
   ],
   "source": [
    "total_reward_episode = reinforce(env,estimator=policy_net,n_episode=n_episode,gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625bce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.model.state_dict(),f'saved_models/saved_rl_model_1_lr_{lr}_gamma_{gamma}_ep_{n_episode}_entropic_factor_{entropic_factor}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bdcf546",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'saved_models/saved_rl_model_1_lr_{lr}_gamma_{gamma}_ep_{n_episode}_entropic_factor_{entropic_factor}.txt',total_reward_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df9557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
